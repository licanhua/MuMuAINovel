"""AIæœåŠ¡å°è£… - ç»Ÿä¸€çš„å¤šæä¾›å•†æ¥å£"""
from typing import Optional, AsyncGenerator, List, Dict, Any, Protocol
from abc import ABC, abstractmethod
from enum import Enum
import httpx
import hashlib
from app.config import settings as app_settings
from app.logger import get_logger

logger = get_logger(__name__)

# ============================================================================
# Provider Enums
# ============================================================================

class AIProvider(str, Enum):
    """AIæä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"
    CUSTOM = "custom"  # è‡ªå®šä¹‰OpenAIå…¼å®¹API


# ============================================================================
# HTTP Client Pool Management
# ============================================================================

_http_client_pool: Dict[str, httpx.AsyncClient] = {}


def _get_client_key(provider: str, base_url: Optional[str], api_key: str) -> str:
    """ç”ŸæˆHTTPå®¢æˆ·ç«¯çš„å”¯ä¸€é”®"""
    key_hash = hashlib.md5(api_key.encode()).hexdigest()[:8]
    url_part = base_url or "default"
    return f"{provider}_{url_part}_{key_hash}"


def _get_or_create_http_client(
    provider: str,
    base_url: Optional[str],
    api_key: str
) -> httpx.AsyncClient:
    """è·å–æˆ–åˆ›å»ºHTTPå®¢æˆ·ç«¯ï¼ˆå¤ç”¨è¿æ¥ï¼‰"""
    global _http_client_pool
    
    client_key = _get_client_key(provider, base_url, api_key)
    
    if client_key in _http_client_pool:
        client = _http_client_pool[client_key]
        if not client.is_closed:
            logger.debug(f"â™»ï¸ å¤ç”¨HTTPå®¢æˆ·ç«¯: {client_key}")
            return client
        else:
            logger.warning(f"âš ï¸ HTTPå®¢æˆ·ç«¯å·²å…³é—­ï¼Œé‡æ–°åˆ›å»º: {client_key}")
            del _http_client_pool[client_key]
    
    limits = httpx.Limits(
        max_keepalive_connections=50,
        max_connections=100,
        keepalive_expiry=30.0
    )
    
    client = httpx.AsyncClient(
        timeout=httpx.Timeout(
            connect=60.0,
            read=180.0,
            write=60.0,
            pool=60.0
        ),
        limits=limits,
        headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
    )
    
    _http_client_pool[client_key] = client
    logger.info(f"âœ… åˆ›å»ºæ–°HTTPå®¢æˆ·ç«¯å¹¶åŠ å…¥æ± : {client_key} (æ± å¤§å°: {len(_http_client_pool)})")
    
    return client


async def cleanup_http_clients():
    """æ¸…ç†æ‰€æœ‰HTTPå®¢æˆ·ç«¯ï¼ˆåº”ç”¨å…³é—­æ—¶è°ƒç”¨ï¼‰"""
    global _http_client_pool
    
    logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†HTTPå®¢æˆ·ç«¯æ±  (å…± {len(_http_client_pool)} ä¸ªå®¢æˆ·ç«¯)")
    
    for key, client in list(_http_client_pool.items()):
        try:
            if not client.is_closed:
                await client.aclose()
                logger.debug(f"âœ… å…³é—­HTTPå®¢æˆ·ç«¯: {key}")
        except Exception as e:
            logger.error(f"âŒ å…³é—­HTTPå®¢æˆ·ç«¯å¤±è´¥ {key}: {e}")
    
    _http_client_pool.clear()
    logger.info("âœ… HTTPå®¢æˆ·ç«¯æ± æ¸…ç†å®Œæˆ")


# ============================================================================
# Provider Interface
# ============================================================================

class AIProviderInterface(ABC):
    """AIæä¾›å•†ç»Ÿä¸€æ¥å£"""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        http_client: Optional[httpx.AsyncClient] = None
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.http_client = http_client
    
    @abstractmethod
    async def generate_text(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
        
        Returns:
            {
                "content": "ç”Ÿæˆçš„æ–‡æœ¬",
                "tool_calls": [...],  # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
                "finish_reason": "stop"
            }
        """
        pass
    
    @abstractmethod
    async def generate_text_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        pass


# ============================================================================
# OpenAI Provider
# ============================================================================

class OpenAIProvider(AIProviderInterface):
    """OpenAIæä¾›å•†å®ç°"""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        http_client: Optional[httpx.AsyncClient] = None
    ):
        super().__init__(api_key, base_url, http_client)
        from openai import AsyncOpenAI
        
        if not http_client:
            http_client = _get_or_create_http_client("openai", base_url, api_key)
        
        client_kwargs = {
            "api_key": api_key,
            "http_client": http_client
        }
        
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = AsyncOpenAI(**client_kwargs)
        logger.info("âœ… OpenAIæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
    
    async def generate_text(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨OpenAI API - æ¨¡å‹: {model}")
            
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # æ·»åŠ å·¥å…·å‚æ•°
            if tools:
                kwargs["tools"] = tools
                if tool_choice:
                    if tool_choice == "required":
                        kwargs["tool_choice"] = "required"
                    elif tool_choice == "auto":
                        kwargs["tool_choice"] = "auto"
                    elif tool_choice == "none":
                        kwargs["tool_choice"] = "none"
            
            response = await self.client.chat.completions.create(**kwargs)
            
            choice = response.choices[0]
            message = choice.message
            
            # æ£€æŸ¥å·¥å…·è°ƒç”¨
            tool_calls = []
            if message.tool_calls:
                for tool_call in message.tool_calls:
                    tool_calls.append({
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    })
            
            return {
                "content": message.content or "",
                "tool_calls": tool_calls if tool_calls else None,
                "finish_reason": choice.finish_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def generate_text_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨OpenAIæµå¼ç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨OpenAIæµå¼API - æ¨¡å‹: {model}")
            
            stream = await self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content
            
        except Exception as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise


# ============================================================================
# Anthropic Provider
# ============================================================================

class AnthropicProvider(AIProviderInterface):
    """Anthropicæä¾›å•†å®ç°"""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        http_client: Optional[httpx.AsyncClient] = None
    ):
        super().__init__(api_key, base_url, http_client)
        from anthropic import AsyncAnthropic
        
        if not http_client:
            http_client = _get_or_create_http_client("anthropic", base_url, api_key)
        
        client_kwargs = {
            "api_key": api_key,
            "http_client": http_client
        }
        
        if base_url:
            client_kwargs["base_url"] = base_url
        
        self.client = AsyncAnthropic(**client_kwargs)
        logger.info("âœ… Anthropicæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
    
    async def generate_text(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Anthropicç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨Anthropic API - æ¨¡å‹: {model}")
            
            # æå–systemæ¶ˆæ¯
            system_prompt = None
            user_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    system_prompt = msg["content"]
                else:
                    user_messages.append(msg)
            
            kwargs = {
                "model": model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": user_messages
            }
            
            if system_prompt:
                kwargs["system"] = system_prompt
            
            # æ·»åŠ å·¥å…·å‚æ•°
            if tools:
                kwargs["tools"] = tools
                if tool_choice == "required":
                    kwargs["tool_choice"] = {"type": "any"}
                elif tool_choice == "auto":
                    kwargs["tool_choice"] = {"type": "auto"}
            
            response = await self.client.messages.create(**kwargs)
            
            # å¤„ç†å“åº”
            tool_calls = []
            content_text = ""
            
            for block in response.content:
                if block.type == "tool_use":
                    tool_calls.append({
                        "id": block.id,
                        "type": "function",
                        "function": {
                            "name": block.name,
                            "arguments": block.input
                        }
                    })
                elif block.type == "text":
                    content_text += block.text
            
            return {
                "content": content_text,
                "tool_calls": tool_calls if tool_calls else None,
                "finish_reason": response.stop_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def generate_text_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Anthropicæµå¼ç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨Anthropicæµå¼API - æ¨¡å‹: {model}")
            
            # æå–systemæ¶ˆæ¯
            system_prompt = None
            user_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    system_prompt = msg["content"]
                else:
                    user_messages.append(msg)
            
            kwargs = {
                "model": model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": user_messages
            }
            
            if system_prompt:
                kwargs["system"] = system_prompt
            
            async with self.client.messages.stream(**kwargs) as stream:
                async for text in stream.text_stream:
                    yield text
            
        except Exception as e:
            logger.error(f"âŒ Anthropicæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise


# ============================================================================
# Gemini Provider
# ============================================================================

class GeminiProvider(AIProviderInterface):
    """Google Geminiæä¾›å•†å®ç°ï¼ˆä½¿ç”¨å®˜æ–¹google-generativeaiåº“ï¼‰"""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        http_client: Optional[httpx.AsyncClient] = None
    ):
        super().__init__(api_key, base_url, http_client)
        
        try:
            import google.generativeai as genai
            
            # é…ç½®APIå¯†é’¥
            genai.configure(api_key=api_key)
            
            self.genai = genai
            logger.info("âœ… Geminiæä¾›å•†åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.error("âŒ æœªå®‰è£…google-generativeaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-generativeai")
            raise ImportError("è¯·å®‰è£…google-generativeai: pip install google-generativeai")
    
    def _convert_messages_to_gemini(
        self,
        messages: List[Dict[str, str]]
    ) -> tuple[Optional[str], List[Dict[str, str]]]:
        """è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
        
        Returns:
            (system_instruction, chat_history)
        """
        system_instruction = None
        chat_history = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_instruction = msg["content"]
            elif msg["role"] == "user":
                chat_history.append({
                    "role": "user",
                    "parts": [msg["content"]]
                })
            elif msg["role"] == "assistant":
                chat_history.append({
                    "role": "model",
                    "parts": [msg["content"]]
                })
        
        return system_instruction, chat_history
    
    def _convert_tools_to_gemini(
        self,
        tools: Optional[List[Dict[str, Any]]]
    ) -> Optional[List]:
        """è½¬æ¢OpenAIå·¥å…·æ ¼å¼ä¸ºGemini Function Callingæ ¼å¼"""
        if not tools:
            return None
        
        gemini_tools = []
        for tool in tools:
            if tool.get("type") == "function":
                func = tool.get("function", {})
                gemini_tools.append({
                    "name": func.get("name"),
                    "description": func.get("description", ""),
                    "parameters": func.get("parameters", {})
                })
        
        return gemini_tools if gemini_tools else None
    
    async def generate_text(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Geminiç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨Gemini API - æ¨¡å‹: {model}")
            
            system_instruction, chat_history = self._convert_messages_to_gemini(messages)
            
            # åˆ›å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
            }
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model_kwargs = {
                "model_name": model,
                "generation_config": generation_config
            }
            
            if system_instruction:
                model_kwargs["system_instruction"] = system_instruction
            
            # è½¬æ¢å·¥å…·æ ¼å¼
            gemini_tools = self._convert_tools_to_gemini(tools)
            if gemini_tools:
                model_kwargs["tools"] = gemini_tools
            
            gemini_model = self.genai.GenerativeModel(**model_kwargs)
            
            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œä½¿ç”¨chatæ¨¡å¼
            if len(chat_history) > 1:
                # æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å½“å‰è¾“å…¥
                current_message = chat_history[-1]["parts"][0]
                history = chat_history[:-1]
                
                chat = gemini_model.start_chat(history=history)
                response = await chat.send_message_async(current_message)
            else:
                # å•æ¡æ¶ˆæ¯ï¼Œç›´æ¥ç”Ÿæˆ
                current_message = chat_history[0]["parts"][0] if chat_history else ""
                response = await gemini_model.generate_content_async(current_message)
            
            # å¤„ç†å“åº”
            content = ""
            tool_calls = []
            
            for candidate in response.candidates:
                for part in candidate.content.parts:
                    if hasattr(part, 'text') and part.text:
                        content += part.text
                    elif hasattr(part, 'function_call') and part.function_call:
                        # Geminiçš„function call
                        fc = part.function_call
                        tool_calls.append({
                            "id": f"call_{hash(fc.name)}",  # Geminiä¸æä¾›call_idï¼Œç”Ÿæˆä¸€ä¸ª
                            "type": "function",
                            "function": {
                                "name": fc.name,
                                "arguments": dict(fc.args)
                            }
                        })
            
            # è·å–finish_reason
            finish_reason = "stop"
            if response.candidates:
                finish_reason = str(response.candidates[0].finish_reason)
            
            return {
                "content": content,
                "tool_calls": tool_calls if tool_calls else None,
                "finish_reason": finish_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ Gemini APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def generate_text_stream(
        self,
        messages: List[Dict[str, str]],
        model: str,
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Geminiæµå¼ç”Ÿæˆæ–‡æœ¬"""
        try:
            logger.info(f"ğŸ”µ è°ƒç”¨Geminiæµå¼API - æ¨¡å‹: {model}")
            
            system_instruction, chat_history = self._convert_messages_to_gemini(messages)
            
            # åˆ›å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
            }
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model_kwargs = {
                "model_name": model,
                "generation_config": generation_config
            }
            
            if system_instruction:
                model_kwargs["system_instruction"] = system_instruction
            
            gemini_model = self.genai.GenerativeModel(**model_kwargs)
            
            # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œä½¿ç”¨chatæ¨¡å¼
            if len(chat_history) > 1:
                current_message = chat_history[-1]["parts"][0]
                history = chat_history[:-1]
                
                chat = gemini_model.start_chat(history=history)
                response = await chat.send_message_async(
                    current_message,
                    stream=True
                )
            else:
                current_message = chat_history[0]["parts"][0] if chat_history else ""
                response = await gemini_model.generate_content_async(
                    current_message,
                    stream=True
                )
            
            # æµå¼è¾“å‡º
            async for chunk in response:
                if chunk.text:
                    yield chunk.text
            
        except Exception as e:
            logger.error(f"âŒ Geminiæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise


# ============================================================================
# Provider Factory
# ============================================================================

class AIProviderFactory:
    """AIæä¾›å•†å·¥å‚"""
    
    @staticmethod
    def create_provider(
        provider: str,
        api_key: str,
        base_url: Optional[str] = None,
        http_client: Optional[httpx.AsyncClient] = None
    ) -> AIProviderInterface:
        """åˆ›å»ºAIæä¾›å•†å®ä¾‹"""
        provider = provider.lower()
        
        if provider == AIProvider.OPENAI or provider == AIProvider.CUSTOM:
            return OpenAIProvider(api_key, base_url, http_client)
        elif provider == AIProvider.ANTHROPIC:
            return AnthropicProvider(api_key, base_url, http_client)
        elif provider == AIProvider.GEMINI:
            return GeminiProvider(api_key, base_url, http_client)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")


# ============================================================================
# Main AI Service
# ============================================================================

class AIService:
    """AIæœåŠ¡ç»Ÿä¸€æ¥å£ - æ”¯æŒå¤šæä¾›å•†"""
    
    def __init__(
        self,
        api_provider: Optional[str] = None,
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: Optional[float] = None,
        default_max_tokens: Optional[int] = None
    ):
        """åˆå§‹åŒ–AIæœåŠ¡"""
        self.api_provider = api_provider or app_settings.default_ai_provider
        self.default_model = default_model or app_settings.default_model
        self.default_temperature = default_temperature or app_settings.default_temperature
        self.default_max_tokens = default_max_tokens or app_settings.default_max_tokens
        
        # åˆå§‹åŒ–æä¾›å•†
        self.providers: Dict[str, AIProviderInterface] = {}
        
        # åˆå§‹åŒ–OpenAI
        openai_key = api_key if api_provider == "openai" else app_settings.openai_api_key
        if openai_key:
            try:
                base_url = api_base_url if api_provider == "openai" else app_settings.openai_base_url
                self.providers["openai"] = AIProviderFactory.create_provider(
                    "openai", openai_key, base_url
                )
            except Exception as e:
                logger.error(f"OpenAIæä¾›å•†åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–Anthropic
        anthropic_key = api_key if api_provider == "anthropic" else app_settings.anthropic_api_key
        if anthropic_key:
            try:
                base_url = api_base_url if api_provider == "anthropic" else app_settings.anthropic_base_url
                self.providers["anthropic"] = AIProviderFactory.create_provider(
                    "anthropic", anthropic_key, base_url
                )
            except Exception as e:
                logger.error(f"Anthropicæä¾›å•†åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–Gemini
        gemini_key = api_key if api_provider == "gemini" else app_settings.gemini_api_key
        if gemini_key:
            try:
                base_url = api_base_url if api_provider == "gemini" else app_settings.gemini_base_url
                self.providers["gemini"] = AIProviderFactory.create_provider(
                    "gemini", gemini_key, base_url
                )
            except Exception as e:
                logger.error(f"Geminiæä¾›å•†åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _get_provider(self, provider: Optional[str] = None) -> AIProviderInterface:
        """è·å–AIæä¾›å•†å®ä¾‹"""
        provider = provider or self.api_provider
        provider = provider.lower()
        
        if provider not in self.providers:
            raise ValueError(f"æä¾›å•† '{provider}' æœªåˆå§‹åŒ–æˆ–ä¸å¯ç”¨")
        
        return self.providers[provider]
    
    def _build_messages(
        self,
        prompt: str,
        system_prompt: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        return messages
    
    async def generate_text(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
        provider_instance = self._get_provider(provider)
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        messages = self._build_messages(prompt, system_prompt)
        
        return await provider_instance.generate_text(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            tools=tools,
            tool_choice=tool_choice
        )
    
    async def generate_text_stream(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆæ–‡æœ¬"""
        provider_instance = self._get_provider(provider)
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        messages = self._build_messages(prompt, system_prompt)
        
        async for chunk in provider_instance.generate_text_stream(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens
        ):
            yield chunk
    
    async def generate_text_with_mcp(
        self,
        prompt: str,
        user_id: str,
        db_session,
        enable_mcp: bool = True,
        max_tool_rounds: int = 3,
        tool_choice: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """æ”¯æŒMCPå·¥å…·çš„AIæ–‡æœ¬ç”Ÿæˆï¼ˆéæµå¼ï¼‰"""
        from app.services.mcp_tool_service import mcp_tool_service, MCPToolServiceError
        
        result = {
            "content": "",
            "tool_calls_made": 0,
            "tools_used": [],
            "finish_reason": "",
            "mcp_enhanced": False
        }
        
        # è·å–MCPå·¥å…·
        tools = None
        if enable_mcp:
            try:
                tools = await mcp_tool_service.get_user_enabled_tools(
                    user_id=user_id,
                    db_session=db_session
                )
                if tools:
                    logger.info(f"MCPå¢å¼º: åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")
                    result["mcp_enhanced"] = True
            except MCPToolServiceError as e:
                logger.error(f"è·å–MCPå·¥å…·å¤±è´¥: {e}")
                tools = None
        
        # å·¥å…·è°ƒç”¨å¾ªç¯
        conversation_history = [{"role": "user", "content": prompt}]
        
        for round_num in range(max_tool_rounds):
            logger.info(f"MCPå·¥å…·è°ƒç”¨è½®æ¬¡: {round_num + 1}/{max_tool_rounds}")
            
            ai_response = await self.generate_text(
                prompt=conversation_history[-1]["content"],
                tools=tools if round_num == 0 else None,
                tool_choice=tool_choice if round_num == 0 else None,
                **kwargs
            )
            
            tool_calls = ai_response.get("tool_calls")
            
            if not tool_calls:
                result["content"] = ai_response.get("content", "")
                result["finish_reason"] = ai_response.get("finish_reason", "stop")
                break
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            logger.info(f"AIè¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
            
            try:
                tool_results = await mcp_tool_service.execute_tool_calls(
                    user_id=user_id,
                    tool_calls=tool_calls,
                    db_session=db_session
                )
                
                for tool_call in tool_calls:
                    tool_name = tool_call["function"]["name"]
                    if tool_name not in result["tools_used"]:
                        result["tools_used"].append(tool_name)
                
                result["tool_calls_made"] += len(tool_calls)
                
                tool_context = await mcp_tool_service.build_tool_context(
                    tool_results,
                    format="markdown"
                )
                
                next_prompt = f"{prompt}\n\n{tool_context}\n\nè¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»§ç»­å®Œæˆä»»åŠ¡ã€‚"
                conversation_history.append({"role": "user", "content": next_prompt})
                
            except Exception as e:
                logger.error(f"æ‰§è¡ŒMCPå·¥å…·å¤±è´¥: {e}", exc_info=True)
                result["content"] = ai_response.get("content", "")
                result["finish_reason"] = "tool_error"
                break
        else:
            logger.warning(f"è¾¾åˆ°MCPæœ€å¤§è°ƒç”¨è½®æ¬¡ {max_tool_rounds}")
            result["content"] = conversation_history[-1].get("content", "")
            result["finish_reason"] = "max_rounds"
        
        return result
    
    async def generate_text_stream_with_mcp(
        self,
        prompt: str,
        user_id: str,
        db_session,
        enable_mcp: bool = True,
        mcp_planning_prompt: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """æ”¯æŒMCPå·¥å…·çš„AIæµå¼æ–‡æœ¬ç”Ÿæˆï¼ˆä¸¤é˜¶æ®µæ¨¡å¼ï¼‰"""
        from app.services.mcp_tool_service import mcp_tool_service
        
        enhanced_prompt = prompt
        
        if enable_mcp:
            try:
                tools = await mcp_tool_service.get_user_enabled_tools(
                    user_id=user_id,
                    db_session=db_session
                )
                
                if tools:
                    logger.info(f"MCPå¢å¼ºï¼ˆæµå¼ï¼‰: åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")
                    
                    if not mcp_planning_prompt:
                        mcp_planning_prompt = (
                            f"ä»»åŠ¡: {prompt}\n\n"
                            f"è¯·åˆ†æè¿™ä¸ªä»»åŠ¡ï¼Œå†³å®šæ˜¯å¦éœ€è¦æŸ¥è¯¢å¤–éƒ¨ä¿¡æ¯ã€‚"
                            f"å¦‚æœéœ€è¦ï¼Œè¯·è°ƒç”¨ç›¸åº”çš„å·¥å…·è·å–ä¿¡æ¯ã€‚"
                        )
                    
                    planning_result = await self.generate_text_with_mcp(
                        prompt=mcp_planning_prompt,
                        user_id=user_id,
                        db_session=db_session,
                        enable_mcp=True,
                        max_tool_rounds=2,
                        tool_choice="auto",
                        **kwargs
                    )
                    
                    if planning_result["tool_calls_made"] > 0:
                        enhanced_prompt = (
                            f"{prompt}\n\n"
                            f"ã€å‚è€ƒèµ„æ–™ã€‘\n"
                            f"{planning_result.get('content', '')}"
                        )
                        logger.info(f"MCPå·¥å…·è§„åˆ’å®Œæˆï¼Œè°ƒç”¨äº† {planning_result['tool_calls_made']} æ¬¡å·¥å…·")
            
            except Exception as e:
                logger.error(f"MCPå·¥å…·è§„åˆ’å¤±è´¥: {e}")
        
        async for chunk in self.generate_text_stream(
            prompt=enhanced_prompt,
            **kwargs
        ):
            yield chunk


# ============================================================================
# Global Instances
# ============================================================================

ai_service = AIService()


def create_user_ai_service(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int
) -> AIService:
    """æ ¹æ®ç”¨æˆ·è®¾ç½®åˆ›å»ºAIæœåŠ¡å®ä¾‹"""
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens
    )